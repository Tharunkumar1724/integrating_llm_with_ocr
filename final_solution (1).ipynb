{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a1d815-b05a-41d0-ad54-7cf8fee3253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1634 chunks from tirumala_hybrid_chunks_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [02:06<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built and saved\n",
      "ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:   Architectural features of the Garbha griha\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: The architectural features of the Garbha griha include a square shape with an inner measurement of about 12 feet 9 inches. Its effective thickness is about seven feet two inches, which is equivalent to five hasthams.\n",
      "üìÇ Sources: ['page_314.json (page None)', 'page_329.json (page None)', 'page_185.json (page None)', 'page_324.json (page None)', 'page_667.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: I don't know.\n",
      "üìÇ Sources: ['page_520.json (page None)', 'page_647.json (page None)', 'page_043.json (page None)', 'page_078.json (page None)', 'page_336.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  south wall of the Ramar Madai \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: On the south wall of the Ramar Madai, there is a complete inscription that extols Vijayaganda Gopala for his charitable disposition and states that every living being was benefited by him and that every one was grateful to him.\n",
      "üìÇ Sources: ['page_327.json (page None)', 'page_277.json (page None)', 'page_102.json (page None)', 'page_359.json (page None)', 'page_622.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was mentioned as the author who added a closed passage called the Vaikuntha pradakshina, which runs around the sanctum of the temple.\n",
      "üìÇ Sources: ['page_520.json (page None)', 'page_647.json (page None)', 'page_043.json (page None)', 'page_078.json (page None)', 'page_336.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:    Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was the author who added a closed passage called the Vaikuntha pradakshina, which runs around the sanctum of the temple.\n",
      "üìÇ Sources: ['page_520.json (page None)', 'page_647.json (page None)', 'page_043.json (page None)', 'page_078.json (page None)', 'page_336.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  where tirupati located\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Tirupati is located in the state of Andhra Pradesh in India.\n",
      "üìÇ Sources: ['page_022.json (page None)', 'page_434.json (page None)', 'page_053.json (page None)', 'page_446.json (page None)', 'page_536.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  who is Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was a benefactor of the Tirumala temple.\n",
      "üìÇ Sources: ['page_647.json (page None)', 'page_355.json (page None)', 'page_336.json (page None)', 'page_050.json (page None)', 'page_506.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  history of Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was a Telugu Pallava chief who was one of the adherents of Undara Pandya. His political life should have commenced much earlier, but the usually assigned date of his rule is 1250-1285 A.D.\n",
      "üìÇ Sources: ['page_327.json (page None)', 'page_050.json (page None)', 'page_472.json (page None)', 'page_078.json (page None)', 'page_520.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 3Ô∏è‚É£ Paths\n",
    "chunk_dir = Path(\"tirumala_hybrid_chunks_final\")\n",
    "\n",
    "# 4Ô∏è‚É£ Load final chunks (with page info if available)\n",
    "chunks = []\n",
    "for file in chunk_dir.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for ch in data.get(\"final_chunks\", []):\n",
    "            chunks.append({\n",
    "                \"text\": ch[\"content\"],\n",
    "                \"source\": file.name,\n",
    "                \"page\": ch.get(\"page\"),   # ‚úÖ store page if available\n",
    "                \"type\": ch.get(\"type\", \"normalized\")\n",
    "            })\n",
    "print(f\"‚úÖ Loaded {len(chunks)} chunks from {chunk_dir}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode([c[\"text\"] for c in chunks], show_progress_bar=True)\n",
    "embeddings = normalize(embeddings, axis=1)  # ‚úÖ normalize for cosine similarity\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# 6Ô∏è‚É£ FAISS HNSW Index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexHNSWFlat(dim, 32)\n",
    "index.hnsw.efSearch = 64\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"tirumala_chunks_hnsw.index\")\n",
    "print(\"‚úÖ FAISS index built and saved\")\n",
    "\n",
    "# 7Ô∏è‚É£ Retriever (returns indices)\n",
    "def retrieve(query, top_k=10):\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = normalize(q_emb, axis=1).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    return [(i, float(D[0][j])) for j, i in enumerate(I[0]) if i >= 0], q_emb\n",
    "\n",
    "# 8Ô∏è‚É£ Deduplication\n",
    "def dedupe(results):\n",
    "    seen, unique = set(), []\n",
    "    for idx, score in results:\n",
    "        text = chunks[idx][\"text\"]\n",
    "        if text not in seen:\n",
    "            seen.add(text)\n",
    "            unique.append((idx, score))\n",
    "    return unique\n",
    "\n",
    "# 9Ô∏è‚É£ Clustering-based pruning (works on indices)\n",
    "def cluster_prune(results, n_clusters=5):\n",
    "    if len(results) <= n_clusters:\n",
    "        return results\n",
    "    idxs = [r[0] for r in results]\n",
    "    vecs = embeddings[idxs]\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=0).fit(vecs)\n",
    "    best = {}\n",
    "    for label, (idx, score) in zip(km.labels_, results):\n",
    "        if label not in best or score > best[label][1]:  # ‚úÖ keep higher similarity\n",
    "            best[label] = (idx, score)\n",
    "    return [(idx, score) for idx, score in best.values()]\n",
    "\n",
    "# üîü Cosine similarity re-ranker\n",
    "def rerank(results, q_emb):\n",
    "    reranked = []\n",
    "    for idx, _ in results:\n",
    "        c_emb = embeddings[idx]\n",
    "        cos_sim = float((q_emb @ c_emb.T).item())  # ‚úÖ safe scalar\n",
    "        reranked.append((idx, cos_sim))\n",
    "    return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Groq LLM setup\n",
    "llm = ChatGroq(\n",
    "    api_key=\"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\",  # ‚ö†Ô∏è replace with your valid key\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Ask function with history\n",
    "def ask(query, history=None, top_k=10):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # 1. retrieve\n",
    "    candidates, q_emb = retrieve(query, top_k=top_k)\n",
    "    candidates = dedupe(candidates)\n",
    "    candidates = cluster_prune(candidates, n_clusters=5)\n",
    "    candidates = rerank(candidates, q_emb)\n",
    "\n",
    "    # 2. build context\n",
    "    context = \"\\n\".join([chunks[idx][\"text\"] for idx, _ in candidates])\n",
    "\n",
    "    # 3. build conversational prompt\n",
    "    history_text = \"\"\n",
    "    for turn in history:\n",
    "        history_text += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful historian chatbot answering strictly based \n",
    "on the Tirumala Hybrid Book.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "- Do not speak to the user as if they are a historical figure.\n",
    "- Always answer in the third person (e.g., \"Ramaraja was the son of Bukkaraja\").\n",
    "- If the context is insufficient, say \"I don't know.\"\n",
    "- Be concise, factual, and neutral.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "User: {query}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    # 4. call LLM\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    return result.content, candidates\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Chat loop\n",
    "if __name__ == \"__main__\":\n",
    "    history = []\n",
    "    print(\"ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"üßë You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response, refs = ask(user_query, history)\n",
    "\n",
    "        print(\"\\nü§ñ Bot:\", response)\n",
    "        print(\"üìÇ Sources:\", [\n",
    "            f\"{chunks[idx]['source']} (page {chunks[idx].get('page')})\"\n",
    "            for idx, _ in refs\n",
    "        ], \"\\n\")\n",
    "\n",
    "        # save turn\n",
    "        history.append({\"user\": user_query, \"assistant\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce0469f-80e5-4f2b-8c8e-c646170750a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1634 chunks from tirumala_hybrid_chunks_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [01:58<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built and saved\n",
      "ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was a Telugu Pallava chief who was one of the adherents of Undara Pandya. He was the ruler from 1250 to 1285 A.D.\n",
      "üìÇ Sources: ['page_447.json (page None)', 'page_327.json (page None)', 'page_422.json (page None)', 'page_050.json (page None)', 'page_520.json (page None)', 'page_647.json (page None)', 'page_043.json (page None)', 'page_450.json (page None)', 'page_078.json (page None)', 'page_336.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Sarika birds \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: I don't know.\n",
      "üìÇ Sources: ['page_026.json (page None)', 'page_026.json (page None)', 'page_155_20250918_215650.json (page None)', 'page_592.json (page None)', 'page_155_20250918_215650.json (page None)', 'page_669.json (page None)', 'page_108.json (page None)', 'page_559.json (page None)', 'page_319.json (page None)', 'page_631.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:   Alvars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: The Alvars were a group of twelve Tamil Vaishnavite saints who lived in South India between the 6th and 9th centuries AD. They were devoted to Lord Vishnu and composed hymns in praise of Him. The Alvars are considered to be the earliest devotees of Lord Vishnu in Tamil literature and their hymns are still sung today in the Tirumala Temple.\n",
      "üìÇ Sources: ['page_068.json (page None)', 'page_067.json (page None)', 'page_068.json (page None)', 'page_622.json (page None)', 'page_290.json (page None)', 'page_066.json (page None)', 'page_066.json (page None)', 'page_292.json (page None)', 'page_292.json (page None)', 'page_292.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 3Ô∏è‚É£ Paths\n",
    "chunk_dir = Path(\"tirumala_hybrid_chunks_final\")\n",
    "\n",
    "# 4Ô∏è‚É£ Load final chunks\n",
    "chunks = []\n",
    "for file in chunk_dir.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for ch in data.get(\"final_chunks\", []):\n",
    "            chunks.append({\n",
    "                \"text\": ch[\"content\"],\n",
    "                \"source\": file.name,\n",
    "                \"page\": ch.get(\"page\"),\n",
    "                \"type\": ch.get(\"type\", \"normalized\")\n",
    "            })\n",
    "print(f\"‚úÖ Loaded {len(chunks)} chunks from {chunk_dir}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode([c[\"text\"] for c in chunks], show_progress_bar=True)\n",
    "embeddings = normalize(embeddings, axis=1)\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# 6Ô∏è‚É£ FAISS HNSW Index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexHNSWFlat(dim, 32)\n",
    "index.hnsw.efSearch = 64\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"tirumala_chunks_hnsw.index\")\n",
    "print(\"‚úÖ FAISS index built and saved\")\n",
    "\n",
    "# 7Ô∏è‚É£ Retriever\n",
    "def retrieve(query, top_k=15):\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = normalize(q_emb, axis=1).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    return [(i, float(D[0][j])) for j, i in enumerate(I[0]) if i >= 0], q_emb\n",
    "\n",
    "# 8Ô∏è‚É£ Deduplication\n",
    "def dedupe(results):\n",
    "    seen, unique = set(), []\n",
    "    for idx, score in results:\n",
    "        text = chunks[idx][\"text\"]\n",
    "        if text not in seen:\n",
    "            seen.add(text)\n",
    "            unique.append((idx, score))\n",
    "    return unique\n",
    "\n",
    "# 9Ô∏è‚É£ MMR-based pruning (replaces KMeans)\n",
    "def mmr(query_vec, results, lambda_param=0.7, top_n=5):\n",
    "    if len(results) <= top_n:\n",
    "        return results\n",
    "\n",
    "    idxs = [r[0] for r in results]\n",
    "    vecs = embeddings[idxs]\n",
    "\n",
    "    selected, remaining = [], list(range(len(idxs)))\n",
    "\n",
    "    # start with the most relevant\n",
    "    selected.append(remaining.pop(0))\n",
    "\n",
    "    while len(selected) < top_n and remaining:\n",
    "        best_idx, best_score = None, -1e9\n",
    "        for i in remaining:\n",
    "            sim_to_query = float(np.dot(query_vec, vecs[i]))\n",
    "            sim_to_selected = max(float(np.dot(vecs[i], vecs[j])) for j in selected)\n",
    "            score = lambda_param * sim_to_query - (1 - lambda_param) * sim_to_selected\n",
    "            if score > best_score:\n",
    "                best_idx, best_score = i, score\n",
    "        selected.append(best_idx)\n",
    "        remaining.remove(best_idx)\n",
    "\n",
    "    return [(idxs[i], results[i][1]) for i in selected]\n",
    "\n",
    "# üîü Re-ranker (cosine similarity)\n",
    "def rerank(results, q_emb):\n",
    "    reranked = []\n",
    "    for idx, _ in results:\n",
    "        c_emb = embeddings[idx]\n",
    "        cos_sim = float((q_emb @ c_emb.T).item())\n",
    "        reranked.append((idx, cos_sim))\n",
    "    return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Groq LLM setup\n",
    "llm = ChatGroq(\n",
    "    api_key=\"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\",  # ‚ö†Ô∏è replace\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Query type check (entity vs descriptive)\n",
    "def is_entity_query(query):\n",
    "    # Short queries with <=3 words are likely names/titles\n",
    "    return len(query.split()) <= 3\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Ask function\n",
    "def ask(query, history=None, top_k=10):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    candidates, q_emb = retrieve(query, top_k=top_k)\n",
    "    candidates = dedupe(candidates)\n",
    "\n",
    "    if is_entity_query(query):\n",
    "        # Entity queries ‚Üí high recall, minimal pruning\n",
    "        candidates = rerank(candidates, q_emb)[:10]\n",
    "    else:\n",
    "        # Descriptive queries ‚Üí MMR for diversity\n",
    "        candidates = mmr(q_emb[0], candidates, top_n=7)\n",
    "        candidates = rerank(candidates, q_emb)\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\".join([chunks[idx][\"text\"] for idx, _ in candidates])\n",
    "\n",
    "    # Looser fallback: if almost nothing retrieved\n",
    "    if len(context.strip()) < 50:\n",
    "        return \"I don't know.\", candidates\n",
    "\n",
    "    # Build conversational prompt\n",
    "    history_text = \"\"\n",
    "    for turn in history:\n",
    "        history_text += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful historian chatbot answering strictly based \n",
    "on the Tirumala Hybrid Book.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "- Do not speak to the user as if they are a historical figure.\n",
    "- Always answer in the third person (e.g., \"Ramaraja was the son of Bukkaraja\").\n",
    "- If the context is insufficient, say \"I don't know.\"\n",
    "- Be concise, factual, and neutral.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "User: {query}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "    return result.content, candidates\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Chat loop\n",
    "if __name__ == \"__main__\":\n",
    "    history = []\n",
    "    print(\"ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"üßë You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response, refs = ask(user_query, history)\n",
    "\n",
    "        print(\"\\nü§ñ Bot:\", response)\n",
    "        print(\"üìÇ Sources:\", [\n",
    "            f\"{chunks[idx]['source']} (page {chunks[idx].get('page')})\"\n",
    "            for idx, _ in refs\n",
    "        ], \"\\n\")\n",
    "\n",
    "        history.append({\"user\": user_query, \"assistant\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429e8d2-110c-49e4-96f2-5e140108754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1634 chunks from tirumala_hybrid_chunks_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [01:48<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built and saved\n",
      "ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Vijayaganda Gopala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Vijayaganda Gopala was a Telugu Pallava chief who was one of the adherents of Undara Pandya. His date of rule is usually assigned to 1250-1285 A.D., but his political life likely commenced much earlier.\n",
      "üìÇ Sources: ['page_447.json (page None)', 'page_327.json (page None)', 'page_422.json (page None)', 'page_050.json (page None)', 'page_520.json (page None)', 'page_647.json (page None)', 'page_043.json (page None)', 'page_450.json (page None)', 'page_078.json (page None)', 'page_336.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Sarika birds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: Sarika birds are not mentioned in the provided context. However, a bird similar to the Sarika, which is the White-Browed Starling, is mentioned as a bird that can be seen in the tract around the Tirumala Temple.\n",
      "üìÇ Sources: ['page_026.json (page None)', 'page_026.json (page None)', 'page_155_20250918_215650.json (page None)', 'page_592.json (page None)', 'page_155_20250918_215650.json (page None)', 'page_669.json (page None)', 'page_108.json (page None)', 'page_559.json (page None)', 'page_319.json (page None)', 'page_631.json (page None)'] \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üßë You:  Alvars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Bot: The Alvars were a group of twelve Vaishnavite saints who lived in South India and were the authors of the Nalayira Prabandham, a collection of four thousand hymns sung in praise of Vishnu and His Avatars.\n",
      "üìÇ Sources: ['page_068.json (page None)', 'page_067.json (page None)', 'page_068.json (page None)', 'page_622.json (page None)', 'page_290.json (page None)', 'page_066.json (page None)', 'page_066.json (page None)', 'page_292.json (page None)', 'page_292.json (page None)', 'page_292.json (page None)'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 3Ô∏è‚É£ Paths\n",
    "chunk_dir = Path(\"tirumala_hybrid_chunks_final\")\n",
    "\n",
    "# 4Ô∏è‚É£ Load final chunks\n",
    "chunks = []\n",
    "for file in chunk_dir.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for ch in data.get(\"final_chunks\", []):\n",
    "            chunks.append({\n",
    "                \"text\": ch[\"content\"],\n",
    "                \"source\": file.name,\n",
    "                \"page\": ch.get(\"page\"),\n",
    "                \"type\": ch.get(\"type\", \"normalized\")\n",
    "            })\n",
    "print(f\"‚úÖ Loaded {len(chunks)} chunks from {chunk_dir}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode([c[\"text\"] for c in chunks], show_progress_bar=True)\n",
    "embeddings = normalize(embeddings, axis=1)\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# 6Ô∏è‚É£ FAISS HNSW Index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexHNSWFlat(dim, 32)\n",
    "index.hnsw.efSearch = 64\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"tirumala_chunks_hnsw.index\")\n",
    "print(\"‚úÖ FAISS index built and saved\")\n",
    "\n",
    "# 7Ô∏è‚É£ Retriever\n",
    "def retrieve(query, top_k=15):\n",
    "    q_emb = embedder.encode([query])\n",
    "    q_emb = normalize(q_emb, axis=1).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    return [(i, float(D[0][j])) for j, i in enumerate(I[0]) if i >= 0], q_emb\n",
    "\n",
    "# 8Ô∏è‚É£ Deduplication\n",
    "def dedupe(results):\n",
    "    seen, unique = set(), []\n",
    "    for idx, score in results:\n",
    "        text = chunks[idx][\"text\"]\n",
    "        if text not in seen:\n",
    "            seen.add(text)\n",
    "            unique.append((idx, score))\n",
    "    return unique\n",
    "\n",
    "# 9Ô∏è‚É£ Clustering-based pruning\n",
    "def cluster_prune(results, n_clusters=5):\n",
    "    if len(results) <= n_clusters:\n",
    "        return results\n",
    "    idxs = [r[0] for r in results]\n",
    "    vecs = embeddings[idxs]\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=0).fit(vecs)\n",
    "    best = {}\n",
    "    for label, (idx, score) in zip(km.labels_, results):\n",
    "        if label not in best or score > best[label][1]:\n",
    "            best[label] = (idx, score)\n",
    "    return [(idx, score) for idx, score in best.values()]\n",
    "\n",
    "# üîü Re-ranker (cosine similarity)\n",
    "def rerank(results, q_emb):\n",
    "    reranked = []\n",
    "    for idx, _ in results:\n",
    "        c_emb = embeddings[idx]\n",
    "        cos_sim = float((q_emb @ c_emb.T).item())\n",
    "        reranked.append((idx, cos_sim))\n",
    "    return sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Groq LLM setup\n",
    "llm = ChatGroq(\n",
    "    api_key=\"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\",  # ‚ö†Ô∏è replace with valid key\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Query type check (entity vs descriptive)\n",
    "def is_entity_query(query):\n",
    "    # Short queries with <=3 words are likely names/titles\n",
    "    return len(query.split()) <= 3\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Ask function\n",
    "def ask(query, history=None, top_k=15):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    candidates, q_emb = retrieve(query, top_k=top_k)\n",
    "    candidates = dedupe(candidates)\n",
    "\n",
    "    if is_entity_query(query):\n",
    "        # Entity queries ‚Üí high recall (no clustering, just rerank)\n",
    "        candidates = rerank(candidates, q_emb)[:10]\n",
    "    else:\n",
    "        # Descriptive queries ‚Üí cluster prune + rerank\n",
    "        candidates = cluster_prune(candidates, n_clusters=5)\n",
    "        candidates = rerank(candidates, q_emb)\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\".join([chunks[idx][\"text\"] for idx, _ in candidates])\n",
    "\n",
    "    # Looser fallback: if almost nothing retrieved\n",
    "    if len(context.strip()) < 50:\n",
    "        return \"I don't know.\", candidates\n",
    "\n",
    "    # Build conversational prompt\n",
    "    history_text = \"\"\n",
    "    for turn in history:\n",
    "        history_text += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful historian chatbot answering strictly based \n",
    "on the Tirumala Hybrid Book.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "- Do not speak to the user as if they are a historical figure.\n",
    "- Always answer in the third person (e.g., \"Ramaraja was the son of Bukkaraja\").\n",
    "- If the context is insufficient, say \"I don't know.\"\n",
    "- Be concise, factual, and neutral.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "User: {query}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "    return result.content, candidates\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Chat loop\n",
    "if __name__ == \"__main__\":\n",
    "    history = []\n",
    "    print(\"ü§ñ Tirumala Hybrid Book Chatbot. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"üßë You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response, refs = ask(user_query, history)\n",
    "\n",
    "        print(\"\\nü§ñ Bot:\", response)\n",
    "        print(\"üìÇ Sources:\", [\n",
    "            f\"{chunks[idx]['source']} (page {chunks[idx].get('page')})\"\n",
    "            for idx, _ in refs\n",
    "        ], \"\\n\")\n",
    "\n",
    "        history.append({\"user\": user_query, \"assistant\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a7b7c-6618-42ec-8e35-9e828d40033c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
